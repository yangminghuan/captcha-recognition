{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "902232f2-d890-4bfa-8648-b385ceac8320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "from torchvision.transforms import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c821493f-d1b0-4d25-ac51-99e70505a7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2023\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2af0c751-0499-4acb-a0e4-13b38e3962fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (112, 112)\n",
    "norm_mean = [0.485, 0.456, 0.406]\n",
    "norm_std = [0.229, 0.224, 0.225]\n",
    "trf = transforms.Compose([\n",
    "    transforms.Resize(img_size), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=norm_mean, std=norm_std)\n",
    "])\n",
    "class ImgDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, img_paths, labels, trf):\n",
    "        self.img_paths = img_paths\n",
    "        self.labels = labels\n",
    "        self.trf = trf\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path = self.img_paths[idx]\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        img = self.trf(img)\n",
    "        target = self.labels[idx]\n",
    "        target = torch.tensor(target, dtype=torch.long)\n",
    "        \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b362c9b6-3141-46ed-be36-09a19e1108a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_df = pd.read_csv('./data/train_img_path.csv')\n",
    "val_df = pd.read_csv('./data/val_img_path.csv')\n",
    "test_df = pd.read_csv('./data/test_img_path.csv')\n",
    "\n",
    "train_dataset = ImgDataset(train_df['img_path'].values, train_df['target'].values, trf)\n",
    "val_dataset = ImgDataset(val_df['img_path'].values, val_df['target'].values, trf)\n",
    "test_dataset = ImgDataset(test_df['img_path'].values, test_df['target'].values, trf)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size//2, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size//2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9165bf06-2917-4893-a00a-ef85a9793d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 112, 112])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for img, label in train_loader:\n",
    "    print(img.shape)\n",
    "    print(label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2666afec-5c81-49a0-b193-08d22c48bdc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 899])\n"
     ]
    }
   ],
   "source": [
    "class ImgClassifyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, class_num, pretrained=None):\n",
    "        super().__init__()\n",
    "        self.model = models.efficientnet_b5(pretrained=False)\n",
    "        # self.model = models.efficientnet_b7(pretrained=False)\n",
    "        if pretrained:\n",
    "            self.model.load_state_dict(torch.load(pretrained))\n",
    "        # self.model.classifier.add_module('3', nn.Linear(1000, class_num))\n",
    "        self.model.classifier[1] = nn.Linear(2048, class_num)\n",
    "        # self.model.classifier[1] = nn.Linear(2560, class_num)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class_num = 899\n",
    "model_path = './model/efficientnet_b5_lukemelas-b6417697.pth'\n",
    "# model_path = None\n",
    "model = ImgClassifyModel(class_num=class_num, pretrained=model_path)\n",
    "\n",
    "inputs = torch.zeros((32, 3, 56, 56))\n",
    "outputs = model(inputs)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6b80638-375e-48aa-8fb8-6647a19fdab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_acc(y, pred_y):\n",
    "    pred_y = pred_y.detach().argmax(dim=-1)\n",
    "    acc = (y == pred_y).cpu().numpy()\n",
    "    return acc.mean()\n",
    "\n",
    "def train(epoch, model, iterator, optimizer, loss_fct, scheduler=None, device='cpu'):\n",
    "    model.train()\n",
    "    step = 0\n",
    "    all_loss = 0\n",
    "    all_acc = 0\n",
    "    for img, label in iterator:\n",
    "        step += 1\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        pred = model(img)\n",
    "        loss = loss_fct(pred, label)\n",
    "        all_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        acc = eval_acc(label, pred)\n",
    "        all_acc += acc\n",
    "    \n",
    "    print(\"Epoch: {}, Train Loss: {:.4f}, Train Acc: {:.4f}\".format(epoch, all_loss / step, all_acc / step))\n",
    "\n",
    "def validate(epoch, model, iterator, loss_fct, device):\n",
    "    model.eval()\n",
    "    step = 0\n",
    "    all_loss = 0\n",
    "    all_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for img, label in iterator:\n",
    "            step += 1\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            pred = model(img)\n",
    "            loss = loss_fct(pred, label)\n",
    "            all_loss += loss.item()\n",
    "\n",
    "            acc = eval_acc(label, pred)\n",
    "            all_acc += acc\n",
    "    \n",
    "    print(\"Epoch: {}, Val Loss: {:.4f}, Val Acc: {:.4f}\".format(epoch, all_loss / step, all_acc / step))\n",
    "    return model, all_loss / step, all_acc / step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50f6ef23-727e-4086-8540-d746907bd6fa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start train=============================\n",
      "Epoch: 1, Train Loss: 6.7634, Train Acc: 0.0090\n",
      "Epoch: 1, Val Loss: 6.9430, Val Acc: 0.0031\n",
      "===========================================\n",
      "Epoch: 2, Train Loss: 6.4127, Train Acc: 0.0380\n",
      "Epoch: 2, Val Loss: 6.1673, Val Acc: 0.0824\n",
      "===========================================\n",
      "Epoch: 3, Train Loss: 5.6817, Train Acc: 0.1256\n",
      "Epoch: 3, Val Loss: 4.9236, Val Acc: 0.2132\n",
      "===========================================\n",
      "Epoch: 4, Train Loss: 4.3129, Train Acc: 0.2947\n",
      "Epoch: 4, Val Loss: 3.0310, Val Acc: 0.4459\n",
      "===========================================\n",
      "Epoch: 5, Train Loss: 2.7447, Train Acc: 0.5046\n",
      "Epoch: 5, Val Loss: 1.6125, Val Acc: 0.6693\n",
      "===========================================\n",
      "Epoch: 6, Train Loss: 1.6824, Train Acc: 0.6654\n",
      "Epoch: 6, Val Loss: 0.8928, Val Acc: 0.7970\n",
      "===========================================\n",
      "Epoch: 7, Train Loss: 0.6219, Train Acc: 0.8852\n",
      "Epoch: 7, Val Loss: 0.2303, Val Acc: 0.9619\n",
      "===========================================\n",
      "Epoch: 8, Train Loss: 0.4058, Train Acc: 0.9410\n",
      "Epoch: 8, Val Loss: 0.1720, Val Acc: 0.9747\n",
      "===========================================\n",
      "Epoch: 9, Train Loss: 0.3350, Train Acc: 0.9568\n",
      "Epoch: 9, Val Loss: 0.1581, Val Acc: 0.9765\n",
      "===========================================\n",
      "Epoch: 10, Train Loss: 0.2855, Train Acc: 0.9647\n",
      "Epoch: 10, Val Loss: 0.1416, Val Acc: 0.9792\n",
      "===========================================\n",
      "Epoch: 11, Train Loss: 0.2618, Train Acc: 0.9713\n",
      "Epoch: 11, Val Loss: 0.1271, Val Acc: 0.9836\n",
      "===========================================\n",
      "Epoch: 12, Train Loss: 0.2211, Train Acc: 0.9760\n",
      "Epoch: 12, Val Loss: 0.1004, Val Acc: 0.9867\n",
      "===========================================\n",
      "Epoch: 13, Train Loss: 0.1749, Train Acc: 0.9850\n",
      "Epoch: 13, Val Loss: 0.0870, Val Acc: 0.9871\n",
      "===========================================\n",
      "Epoch: 14, Train Loss: 0.1557, Train Acc: 0.9880\n",
      "Epoch: 14, Val Loss: 0.0832, Val Acc: 0.9867\n",
      "===========================================\n",
      "Epoch: 15, Train Loss: 0.1607, Train Acc: 0.9867\n",
      "Epoch: 15, Val Loss: 0.0834, Val Acc: 0.9876\n",
      "===========================================\n",
      "Epoch: 16, Train Loss: 0.1567, Train Acc: 0.9886\n",
      "Epoch: 16, Val Loss: 0.0828, Val Acc: 0.9885\n",
      "===========================================\n",
      "Epoch: 17, Train Loss: 0.1548, Train Acc: 0.9876\n",
      "Epoch: 17, Val Loss: 0.0830, Val Acc: 0.9885\n",
      "===========================================\n",
      "Epoch: 18, Train Loss: 0.1534, Train Acc: 0.9889\n",
      "Epoch: 18, Val Loss: 0.0789, Val Acc: 0.9880\n",
      "===========================================\n",
      "Epoch: 19, Train Loss: 0.1507, Train Acc: 0.9882\n",
      "Epoch: 19, Val Loss: 0.0775, Val Acc: 0.9885\n",
      "===========================================\n",
      "Epoch: 20, Train Loss: 0.1475, Train Acc: 0.9899\n",
      "Epoch: 20, Val Loss: 0.0774, Val Acc: 0.9880\n",
      "===========================================\n",
      "Epoch: 21, Train Loss: 0.1497, Train Acc: 0.9898\n",
      "Epoch: 21, Val Loss: 0.0773, Val Acc: 0.9880\n",
      "===========================================\n",
      "Epoch: 22, Train Loss: 0.1529, Train Acc: 0.9881\n",
      "Epoch: 22, Val Loss: 0.0778, Val Acc: 0.9885\n",
      "===========================================\n",
      "Epoch: 23, Train Loss: 0.1505, Train Acc: 0.9882\n",
      "Epoch: 23, Val Loss: 0.0790, Val Acc: 0.9880\n",
      "===========================================\n",
      "Epoch: 24, Train Loss: 0.1520, Train Acc: 0.9888\n",
      "Epoch: 24, Val Loss: 0.0783, Val Acc: 0.9885\n",
      "===========================================\n",
      "Epoch: 25, Train Loss: 0.1496, Train Acc: 0.9891\n",
      "Epoch: 25, Val Loss: 0.0781, Val Acc: 0.9885\n",
      "===========================================\n",
      "Epoch: 26, Train Loss: 0.1481, Train Acc: 0.9886\n",
      "Epoch: 26, Val Loss: 0.0785, Val Acc: 0.9880\n",
      "===========================================\n",
      "Epoch: 27, Train Loss: 0.1499, Train Acc: 0.9894\n",
      "Epoch: 27, Val Loss: 0.0782, Val Acc: 0.9880\n",
      "===========================================\n",
      "Epoch: 28, Train Loss: 0.1522, Train Acc: 0.9894\n",
      "Epoch: 28, Val Loss: 0.0782, Val Acc: 0.9889\n",
      "===========================================\n",
      "Epoch: 29, Train Loss: 0.1503, Train Acc: 0.9899\n",
      "Epoch: 29, Val Loss: 0.0782, Val Acc: 0.9880\n",
      "===========================================\n",
      "Epoch: 30, Train Loss: 0.1562, Train Acc: 0.9895\n",
      "Epoch: 30, Val Loss: 0.0784, Val Acc: 0.9880\n",
      "===========================================\n",
      "train finish=============================\n",
      "CPU times: total: 5h 19min 48s\n",
      "Wall time: 2h 53min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "epochs = 30\n",
    "lr = 0.0005\n",
    "# opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "opt = torch.optim.RMSprop(model.parameters(), alpha=0.9, momentum=0.9, lr=lr, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer=opt, step_size=6, gamma=0.1)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "best_model = None\n",
    "best_val_loss = 1e10\n",
    "best_val_acc = 1e-10\n",
    "\n",
    "print(\"start train=============================\")\n",
    "for epoch in range(1, epochs+1):\n",
    "    train(epoch, model=model, iterator=train_loader, loss_fct=loss_func, optimizer=opt, device=device)\n",
    "    candidate_model, loss, acc = validate(epoch, model=model, iterator=val_loader, loss_fct=loss_func, device=device)\n",
    "    scheduler.step()\n",
    "    \n",
    "    if loss < best_val_loss and acc > best_val_acc:\n",
    "        best_model = candidate_model\n",
    "        best_val_loss, best_val_acc = loss, acc\n",
    "    \n",
    "    print(\"===========================================\")\n",
    "print(\"train finish=============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73df0c59-cd6d-475b-92c9-3460d60a6fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9920212765957447"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.eval()\n",
    "test_acc = 0\n",
    "step = 0\n",
    "with torch.no_grad():\n",
    "    for img, label in test_loader:\n",
    "        step += 1\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        pred = best_model(img)\n",
    "        \n",
    "        acc = eval_acc(label, pred)\n",
    "        test_acc += acc\n",
    "\n",
    "test_acc / step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f34bc72-a2f5-4b95-ba5a-58c54400af39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9889184397163121"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f882a85-cbda-4195-b72b-ae80f26349ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.44"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2244 * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87e16ced-aaf3-4dfd-adaf-fa52b6cc44bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(best_model.state_dict(), \"./model/model_0802.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73534a2c-a8f0-480d-be94-5cee9522596d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1aa951d-9f9e-461c-8845-8d4ccfdf70b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = os.listdir('./classes/')\n",
    "label_to_char = dict(enumerate(chars))\n",
    "char_to_label = dict([(j, i) for i, j in label_to_char.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37f2755b-11cc-45a7-8f31-bb1612d8f9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs = None\n",
    "for i in os.listdir('./test/'):\n",
    "    img = Image.open(f'./test/{i}').convert('RGB')\n",
    "    img = trf(img)\n",
    "    img = img.unsqueeze(0)\n",
    "    if test_imgs is None:\n",
    "        test_imgs = img\n",
    "    else:\n",
    "        test_imgs = torch.cat((test_imgs, img), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f13f21da-af04-4edf-b532-13a21cf91bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "test_imgs = test_imgs.to(device)\n",
    "best_model = ImgClassifyModel(class_num=class_num, pretrained=None)\n",
    "best_model.load_state_dict(torch.load('./model/model_0802.pt'))\n",
    "best_model = best_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88859862-2cd9-46f7-a51d-a00a5026a6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.eval()\n",
    "pred_y = best_model(test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a625208-3090-4225-8050-7d8d1b3ad655",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = pred_y.detach().argmax(dim=-1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a32d577-9092-436e-b858-2c7f786d2425",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [label_to_char[y] for y in pred_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33d6851-5a0b-4ff7-b13f-36700d3c11a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfde604-5431-48fb-bb75-d050c03edd6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spider",
   "language": "python",
   "name": "spider"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
